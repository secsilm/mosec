{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Model Serving made Efficient in the Cloud. Introduction \u00a4 Mosec is a high-performance and flexible model serving framework for building ML model-enabled backend and microservices. It bridges the gap between any machine learning models you just trained and the efficient online service API. Highly performant : web layer and task coordination built with Rust \ud83e\udd80, which offers blazing speed in addition to efficient CPU utilization powered by async I/O Ease of use : user interface purely in Python \ud83d\udc0d, by which users can serve their models in an ML framework-agnostic manner using the same code as they do for offline testing Dynamic batching : aggregate requests from different users for batched inference and distribute results back Pipelined stages : spawn multiple processes for pipelined stages to handle CPU/GPU/IO mixed workloads Cloud friendly : designed to run in the cloud, with the model warmup, graceful shutdown, and Prometheus monitoring metrics, easily managed by Kubernetes or any container orchestration systems Do one thing well : focus on the online serving part, users can pay attention to the model performance and business logic Installation \u00a4 Mosec requires Python 3.6 or above. Install the latest PyPI package with: pip install -U mosec Usage \u00a4 Write the server \u00a4 Import the libraries and set up a basic logger to better observe what happens. import logging from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) Then, we build an API to calculate the exponential with base e for a given number. To achieve that, we simply inherit the Worker class and override the forward method. Note that the input req is by default a JSON-decoded object, e.g., a dictionary here (wishfully it receives data like {\"x\": 1} ). We also enclose the input parsing part with a try...except... block to reject invalid input (e.g., no key named \"x\" or field \"x\" cannot be converted to float ). import math class CalculateExp ( Worker ): def forward ( self , req : dict ) -> dict : try : x = float ( req [ \"x\" ]) except KeyError : raise ValidationError ( \"cannot find key 'x'\" ) except ValueError : raise ValidationError ( \"cannot convert 'x' value to float\" ) y = math . exp ( x ) # f(x) = e ^ x logger . debug ( f \"e ^ { x } = { y } \" ) return { \"y\" : y } Finally, we append the worker to the server to construct a single-stage workflow , and we specify the number of processes we want it to run in parallel. Then we run the server. if __name__ == \"__main__\" : server = Server () server . append_worker ( CalculateExp , num = 2 ) # we spawn two processes for parallel computing server . run () Run the server \u00a4 After merging the snippets above into a file named server.py , we can first have a look at the command line arguments: python server.py --help Then let's start the server... python server.py and in another terminal, test it: curl -X POST http://127.0.0.1:8000/inference -d '{\"x\": 2}' or check the metrics: curl http://127.0.0.1:8000/metrics That's it! You have just hosted your exponential-computing model as a server! \ud83d\ude09 Example \u00a4 More ready-to-use examples can be found in the Example section. It includes: Multi-stage workflow Batch processing worker PyTorch deep learning models: sentiment analysis image recognition Qualitative Comparison * \u00a4 Batcher Pipeline Parallel I/O Format (1) Framework (2) Backend Activity TF Serving \u2705 \u2705 \u2705 Limited (a) Heavily TF C++ Triton \u2705 \u2705 \u2705 Limited Multiple C++ MMS \u2705 \u274c \u2705 Limited Heavily MX Java BentoML \u2705 \u274c \u274c Limited (b) Multiple Python Streamer \u2705 \u274c \u2705 Customizable Agnostic Python Flask (3) \u274c \u274c \u274c Customizable Agnostic Python Mosec \u2705 \u2705 \u2705 Customizable Agnostic Rust *As accessed on 08 Oct 2021. By no means is this comparison showing that other frameworks are inferior, but rather it is used to illustrate the trade-off. The information is not guaranteed to be absolutely accurate. Please let us know if you find anything that may be incorrect. (1) : Data format of the service's request and response. \"Limited\" in the sense that the framework has pre-defined requirements on the format. (2) : Supported machine learning frameworks. \"Heavily\" means the serving framework is designed towards a specific ML framework. Thus it is hard, if not impossible, to adapt to others. \"Multiple\" means the serving framework provides adaptation to several existing ML frameworks. \"Agnostic\" means the serving framework does not necessarily care about the ML framework. Hence it supports all ML frameworks (in Python). (3) : Flask is a representative of general purpose web frameworks to host ML models. Contributing \u00a4 We welcome any kind of contribution. Please give us feedback by raising issues or directly contribute your code and pull request!","title":"Overview"},{"location":"#introduction","text":"Mosec is a high-performance and flexible model serving framework for building ML model-enabled backend and microservices. It bridges the gap between any machine learning models you just trained and the efficient online service API. Highly performant : web layer and task coordination built with Rust \ud83e\udd80, which offers blazing speed in addition to efficient CPU utilization powered by async I/O Ease of use : user interface purely in Python \ud83d\udc0d, by which users can serve their models in an ML framework-agnostic manner using the same code as they do for offline testing Dynamic batching : aggregate requests from different users for batched inference and distribute results back Pipelined stages : spawn multiple processes for pipelined stages to handle CPU/GPU/IO mixed workloads Cloud friendly : designed to run in the cloud, with the model warmup, graceful shutdown, and Prometheus monitoring metrics, easily managed by Kubernetes or any container orchestration systems Do one thing well : focus on the online serving part, users can pay attention to the model performance and business logic","title":"Introduction"},{"location":"#installation","text":"Mosec requires Python 3.6 or above. Install the latest PyPI package with: pip install -U mosec","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#write-the-server","text":"Import the libraries and set up a basic logger to better observe what happens. import logging from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) Then, we build an API to calculate the exponential with base e for a given number. To achieve that, we simply inherit the Worker class and override the forward method. Note that the input req is by default a JSON-decoded object, e.g., a dictionary here (wishfully it receives data like {\"x\": 1} ). We also enclose the input parsing part with a try...except... block to reject invalid input (e.g., no key named \"x\" or field \"x\" cannot be converted to float ). import math class CalculateExp ( Worker ): def forward ( self , req : dict ) -> dict : try : x = float ( req [ \"x\" ]) except KeyError : raise ValidationError ( \"cannot find key 'x'\" ) except ValueError : raise ValidationError ( \"cannot convert 'x' value to float\" ) y = math . exp ( x ) # f(x) = e ^ x logger . debug ( f \"e ^ { x } = { y } \" ) return { \"y\" : y } Finally, we append the worker to the server to construct a single-stage workflow , and we specify the number of processes we want it to run in parallel. Then we run the server. if __name__ == \"__main__\" : server = Server () server . append_worker ( CalculateExp , num = 2 ) # we spawn two processes for parallel computing server . run ()","title":"Write the server"},{"location":"#run-the-server","text":"After merging the snippets above into a file named server.py , we can first have a look at the command line arguments: python server.py --help Then let's start the server... python server.py and in another terminal, test it: curl -X POST http://127.0.0.1:8000/inference -d '{\"x\": 2}' or check the metrics: curl http://127.0.0.1:8000/metrics That's it! You have just hosted your exponential-computing model as a server! \ud83d\ude09","title":"Run the server"},{"location":"#example","text":"More ready-to-use examples can be found in the Example section. It includes: Multi-stage workflow Batch processing worker PyTorch deep learning models: sentiment analysis image recognition","title":"Example"},{"location":"#qualitative-comparison","text":"Batcher Pipeline Parallel I/O Format (1) Framework (2) Backend Activity TF Serving \u2705 \u2705 \u2705 Limited (a) Heavily TF C++ Triton \u2705 \u2705 \u2705 Limited Multiple C++ MMS \u2705 \u274c \u2705 Limited Heavily MX Java BentoML \u2705 \u274c \u274c Limited (b) Multiple Python Streamer \u2705 \u274c \u2705 Customizable Agnostic Python Flask (3) \u274c \u274c \u274c Customizable Agnostic Python Mosec \u2705 \u2705 \u2705 Customizable Agnostic Rust *As accessed on 08 Oct 2021. By no means is this comparison showing that other frameworks are inferior, but rather it is used to illustrate the trade-off. The information is not guaranteed to be absolutely accurate. Please let us know if you find anything that may be incorrect. (1) : Data format of the service's request and response. \"Limited\" in the sense that the framework has pre-defined requirements on the format. (2) : Supported machine learning frameworks. \"Heavily\" means the serving framework is designed towards a specific ML framework. Thus it is hard, if not impossible, to adapt to others. \"Multiple\" means the serving framework provides adaptation to several existing ML frameworks. \"Agnostic\" means the serving framework does not necessarily care about the ML framework. Hence it supports all ML frameworks (in Python). (3) : Flask is a representative of general purpose web frameworks to host ML models.","title":"Qualitative Comparison*"},{"location":"#contributing","text":"We welcome any kind of contribution. Please give us feedback by raising issues or directly contribute your code and pull request!","title":"Contributing"},{"location":"argument/","text":"usage: python your_model_server.py [-h] [--path PATH] [--capacity CAPACITY] [--timeout TIMEOUT] [--wait WAIT] [--address ADDRESS] [--port PORT] [--namespace NAMESPACE] Mosec Server Configurations optional arguments: -h, --help show this help message and exit --path PATH Unix Domain Socket address for internal Inter-Process Communication (default: /tmp/mosec) --capacity CAPACITY Capacity of the request queue, beyond which new requests will be rejected with status 429 (default: 1024) --timeout TIMEOUT Service timeout for one request (milliseconds) (default: 3000) --wait WAIT Wait time for the batcher to batch (milliseconds) (default: 10) --address ADDRESS Address of the HTTP service (default: 0.0.0.0) --port PORT Port of the HTTP service (default: 8000) --namespace NAMESPACE Namespace for prometheus metrics (default: mosec_service)","title":"Argument"},{"location":"contributing/","text":"Before contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change. Pull Request Process \u00a4 After you have forked this repository, you could use make install for the first time to install the local development dependencies; afterward, you may use make dev to build the library when you have made any code changes. Before committing your changes, you can use make format && make lint to ensure the codes follow our style standards. Please add corresponding tests to your change if that's related to new feature or API, and ensure make test can pass. Submit your pull request. Contacts \u00a4 Keming kemingy94@gmail.com zclzc lkevinzc@gmail.com","title":"Contributing"},{"location":"contributing/#pull-request-process","text":"After you have forked this repository, you could use make install for the first time to install the local development dependencies; afterward, you may use make dev to build the library when you have made any code changes. Before committing your changes, you can use make format && make lint to ensure the codes follow our style standards. Please add corresponding tests to your change if that's related to new feature or API, and ensure make test can pass. Submit your pull request.","title":"Pull Request Process"},{"location":"contributing/#contacts","text":"Keming kemingy94@gmail.com zclzc lkevinzc@gmail.com","title":"Contacts"},{"location":"interface/","text":"mosec.worker \u00a4 mosec.worker.Worker ( ABC ) \u00a4 This public class defines the mosec worker interface. It provides default IPC (de)serialization methods, stores the worker meta data including its stage and maximum batch size, and leaves the forward method to be implemented by the users. By default, we use JSON encoding. But users are free to customize via simply overridding the deserialize method in the first stage (we term it as ingress stage) and/or the serialize method in the last stage (we term it as egress stage). For the encoding customization, there are many choices including MessagePack , Protocol Buffer and many other out-of-the-box protocols. Users can even define their own protocol and use it to manipulate the raw bytes! A naive customization can be found in this example . Note \u00a4 The \" same type \" mentioned below is applicable only when the stage disables batching. For a stage that enables batching , the worker 's forward should accept a list and output a list, where each element will follow the \" same type \" constraint. mosec . worker . Worker . id : int property readonly \u00a4 This property returns the worker id in the range of [1, ... , num ] ( num as defined here ) to differentiate workers in the same stage. mosec . worker . Worker . deserialize ( self , data : bytes ) -> Any \u00a4 This method defines the deserialization of the first stage (ingress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data bytes the raw bytes extracted from the request body required Returns: Type Description Any the *same type as the argument of the forward you implement Source code in mosec/worker.py def deserialize ( self , data : bytes ) -> Any : \"\"\" This method defines the deserialization of the first stage (ingress). No need to override this method by default, but overridable. Arguments: data: the raw bytes extracted from the request body Returns: the [_*same type_][mosec.worker.Worker--note] as the argument of the `forward` you implement \"\"\" try : data_json = json . loads ( data ) if data else {} except Exception as err : raise DecodingError ( err ) return data_json mosec . worker . Worker . forward ( self , data : Any ) -> Any \u00a4 This method defines the worker's main logic, be it data processing, computation or model inference. Must be overridden by the subclass. The implementation should make sure: (for a single-stage worker) both the input and output follow the *same type rule. (for a multi-stage worker) the input of the ingress stage and the output of the egress stage follow the *same type , while others should align with its adjacent stage's in/output. If any code in the forward needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the __init__ method. Source code in mosec/worker.py @abc . abstractmethod def forward ( self , data : Any ) -> Any : \"\"\" This method defines the worker's main logic, be it data processing, computation or model inference. __Must be overridden__ by the subclass. The implementation should make sure: - (for a single-stage worker) - both the input and output follow the [_*same type_][mosec.worker.Worker--note] rule. - (for a multi-stage worker) - the input of the _ingress_ stage and the output of the _egress_ stage follow the [_*same type_][mosec.worker.Worker--note], while others should align with its adjacent stage's in/output. If any code in the `forward` needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the `__init__` method. \"\"\" raise NotImplementedError mosec . worker . Worker . serialize ( self , data : Any ) -> bytes \u00a4 This method defines serialization of the last stage (egress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data Any the *same type as the output of the forward you implement required Returns: Type Description bytes the bytes you want to put into the response body Source code in mosec/worker.py def serialize ( self , data : Any ) -> bytes : \"\"\" This method defines serialization of the last stage (egress). No need to override this method by default, but overridable. Arguments: data: the [_*same type_][mosec.worker.Worker--note] as the output of the `forward` you implement Returns: the bytes you want to put into the response body \"\"\" try : data_bytes = json . dumps ( data , indent = 2 ) . encode () except Exception as err : raise ValueError ( err ) return data_bytes mosec.server \u00a4 mosec.server.Server \u00a4 This public class defines the mosec server interface. It allows users to sequentially append workers they implemented, builds the workflow pipeline automatically and starts up the server. Batching \u00a4 The user may enable the batching feature for any stage when the corresponding worker is appended, by setting the max_batch_size . Multiprocess \u00a4 The user may spawn multiple processes for any stage when the corresponding worker is appended, by setting the num . mosec . server . Server . append_worker ( self , worker : Type [ mosec . worker . Worker ], num : int = 1 , max_batch_size : int = 1 , start_method : str = 'spawn' , env : Union [ NoneType , List [ Dict [ str , str ]]] = None ) \u00a4 This method sequentially appends workers to the workflow pipeline. Parameters: Name Type Description Default worker Type[mosec.worker.Worker] the class you inherit from Worker which implements the forward method required num int the number of processes for parallel computing (>=1) 1 max_batch_size int the maximum batch size allowed (>=1) 1 start_method str the process starting method (\"spawn\" or \"fork\") 'spawn' env Union[NoneType, List[Dict[str, str]]] the environment variables to set before starting the process None Source code in mosec/server.py def append_worker ( self , worker : Type [ Worker ], num : int = 1 , max_batch_size : int = 1 , start_method : str = \"spawn\" , env : Union [ None , List [ Dict [ str , str ]]] = None , ): \"\"\" This method sequentially appends workers to the workflow pipeline. Arguments: worker: the class you inherit from `Worker` which implements the `forward` method num: the number of processes for parallel computing (>=1) max_batch_size: the maximum batch size allowed (>=1) start_method: the process starting method (\"spawn\" or \"fork\") env: the environment variables to set before starting the process \"\"\" self . _validate_arguments ( worker , num , max_batch_size , start_method , env ) self . _worker_cls . append ( worker ) self . _worker_num . append ( num ) self . _worker_mbs . append ( max_batch_size ) self . _coordinator_env . append ( env ) self . _coordinator_ctx . append ( start_method ) self . _coordinator_pools . append ([ None ] * num ) mosec . server . Server . run ( self ) \u00a4 This method starts the mosec model server! Source code in mosec/server.py def run ( self ): \"\"\" This method starts the mosec model server! \"\"\" self . _validate_server () self . _parse_args () self . _start_controller () try : self . _manage_coordinators () except Exception : logger . error ( traceback . format_exc () . replace ( \" \\n \" , \" \" )) self . _halt () mosec.errors \u00a4 Suppose the input dataflow of our model server is as follows: bytes --- deserialize (decoding) ---> data --- parse (validation) ---> valid data If the raw bytes cannot be successfully deserialized, the DecodingError is raised; if the decoded data cannot pass the validation check (usually implemented by users), the ValidationError should be raised. mosec.errors.DecodingError ( Exception ) \u00a4 The DecodingError should be raised in user-implemented codes when the de-serialization for the request bytes fails. This error will set the status code to HTTP 400 in the response. mosec.errors.ValidationError ( Exception ) \u00a4 The ValidationError should be raised in user-implemented codes, where the validation for the input data fails. Usually, it should be put after the data de-serialization, which converts the raw bytes into structured data. This error will set the status code to HTTP 422 in the response.","title":"Interface"},{"location":"interface/#mosec.worker","text":"","title":"worker"},{"location":"interface/#mosec.worker.Worker","text":"This public class defines the mosec worker interface. It provides default IPC (de)serialization methods, stores the worker meta data including its stage and maximum batch size, and leaves the forward method to be implemented by the users. By default, we use JSON encoding. But users are free to customize via simply overridding the deserialize method in the first stage (we term it as ingress stage) and/or the serialize method in the last stage (we term it as egress stage). For the encoding customization, there are many choices including MessagePack , Protocol Buffer and many other out-of-the-box protocols. Users can even define their own protocol and use it to manipulate the raw bytes! A naive customization can be found in this example .","title":"Worker"},{"location":"interface/#mosec.worker.Worker--note","text":"The \" same type \" mentioned below is applicable only when the stage disables batching. For a stage that enables batching , the worker 's forward should accept a list and output a list, where each element will follow the \" same type \" constraint.","title":"Note"},{"location":"interface/#mosec.worker.Worker.id","text":"This property returns the worker id in the range of [1, ... , num ] ( num as defined here ) to differentiate workers in the same stage.","title":"id"},{"location":"interface/#mosec.worker.Worker.deserialize","text":"This method defines the deserialization of the first stage (ingress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data bytes the raw bytes extracted from the request body required Returns: Type Description Any the *same type as the argument of the forward you implement Source code in mosec/worker.py def deserialize ( self , data : bytes ) -> Any : \"\"\" This method defines the deserialization of the first stage (ingress). No need to override this method by default, but overridable. Arguments: data: the raw bytes extracted from the request body Returns: the [_*same type_][mosec.worker.Worker--note] as the argument of the `forward` you implement \"\"\" try : data_json = json . loads ( data ) if data else {} except Exception as err : raise DecodingError ( err ) return data_json","title":"deserialize()"},{"location":"interface/#mosec.worker.Worker.forward","text":"This method defines the worker's main logic, be it data processing, computation or model inference. Must be overridden by the subclass. The implementation should make sure: (for a single-stage worker) both the input and output follow the *same type rule. (for a multi-stage worker) the input of the ingress stage and the output of the egress stage follow the *same type , while others should align with its adjacent stage's in/output. If any code in the forward needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the __init__ method. Source code in mosec/worker.py @abc . abstractmethod def forward ( self , data : Any ) -> Any : \"\"\" This method defines the worker's main logic, be it data processing, computation or model inference. __Must be overridden__ by the subclass. The implementation should make sure: - (for a single-stage worker) - both the input and output follow the [_*same type_][mosec.worker.Worker--note] rule. - (for a multi-stage worker) - the input of the _ingress_ stage and the output of the _egress_ stage follow the [_*same type_][mosec.worker.Worker--note], while others should align with its adjacent stage's in/output. If any code in the `forward` needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the `__init__` method. \"\"\" raise NotImplementedError","title":"forward()"},{"location":"interface/#mosec.worker.Worker.serialize","text":"This method defines serialization of the last stage (egress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data Any the *same type as the output of the forward you implement required Returns: Type Description bytes the bytes you want to put into the response body Source code in mosec/worker.py def serialize ( self , data : Any ) -> bytes : \"\"\" This method defines serialization of the last stage (egress). No need to override this method by default, but overridable. Arguments: data: the [_*same type_][mosec.worker.Worker--note] as the output of the `forward` you implement Returns: the bytes you want to put into the response body \"\"\" try : data_bytes = json . dumps ( data , indent = 2 ) . encode () except Exception as err : raise ValueError ( err ) return data_bytes","title":"serialize()"},{"location":"interface/#mosec.server","text":"","title":"server"},{"location":"interface/#mosec.server.Server","text":"This public class defines the mosec server interface. It allows users to sequentially append workers they implemented, builds the workflow pipeline automatically and starts up the server.","title":"Server"},{"location":"interface/#mosec.server.Server--batching","text":"The user may enable the batching feature for any stage when the corresponding worker is appended, by setting the max_batch_size .","title":"Batching"},{"location":"interface/#mosec.server.Server--multiprocess","text":"The user may spawn multiple processes for any stage when the corresponding worker is appended, by setting the num .","title":"Multiprocess"},{"location":"interface/#mosec.server.Server.append_worker","text":"This method sequentially appends workers to the workflow pipeline. Parameters: Name Type Description Default worker Type[mosec.worker.Worker] the class you inherit from Worker which implements the forward method required num int the number of processes for parallel computing (>=1) 1 max_batch_size int the maximum batch size allowed (>=1) 1 start_method str the process starting method (\"spawn\" or \"fork\") 'spawn' env Union[NoneType, List[Dict[str, str]]] the environment variables to set before starting the process None Source code in mosec/server.py def append_worker ( self , worker : Type [ Worker ], num : int = 1 , max_batch_size : int = 1 , start_method : str = \"spawn\" , env : Union [ None , List [ Dict [ str , str ]]] = None , ): \"\"\" This method sequentially appends workers to the workflow pipeline. Arguments: worker: the class you inherit from `Worker` which implements the `forward` method num: the number of processes for parallel computing (>=1) max_batch_size: the maximum batch size allowed (>=1) start_method: the process starting method (\"spawn\" or \"fork\") env: the environment variables to set before starting the process \"\"\" self . _validate_arguments ( worker , num , max_batch_size , start_method , env ) self . _worker_cls . append ( worker ) self . _worker_num . append ( num ) self . _worker_mbs . append ( max_batch_size ) self . _coordinator_env . append ( env ) self . _coordinator_ctx . append ( start_method ) self . _coordinator_pools . append ([ None ] * num )","title":"append_worker()"},{"location":"interface/#mosec.server.Server.run","text":"This method starts the mosec model server! Source code in mosec/server.py def run ( self ): \"\"\" This method starts the mosec model server! \"\"\" self . _validate_server () self . _parse_args () self . _start_controller () try : self . _manage_coordinators () except Exception : logger . error ( traceback . format_exc () . replace ( \" \\n \" , \" \" )) self . _halt ()","title":"run()"},{"location":"interface/#mosec.errors","text":"Suppose the input dataflow of our model server is as follows: bytes --- deserialize (decoding) ---> data --- parse (validation) ---> valid data If the raw bytes cannot be successfully deserialized, the DecodingError is raised; if the decoded data cannot pass the validation check (usually implemented by users), the ValidationError should be raised.","title":"errors"},{"location":"interface/#mosec.errors.DecodingError","text":"The DecodingError should be raised in user-implemented codes when the de-serialization for the request bytes fails. This error will set the status code to HTTP 400 in the response.","title":"DecodingError"},{"location":"interface/#mosec.errors.ValidationError","text":"The ValidationError should be raised in user-implemented codes, where the validation for the input data fails. Usually, it should be put after the data de-serialization, which converts the raw bytes into structured data. This error will set the status code to HTTP 422 in the response.","title":"ValidationError"},{"location":"design/","text":"We will explain the detail of our designs in this section.","title":"Overview"},{"location":"example/","text":"We provide examples across different ML frameworks and for various tasks in this section. Get started \u00a4 All the examples in this section are self-contained and tested. Feel free to grab one and run: python model_server.py To test the server, we use httpie and httpx by default. You can have other choices but if you want to install them: pip install httpie httpx","title":"Overview"},{"location":"example/#get-started","text":"All the examples in this section are self-contained and tested. Feel free to grab one and run: python model_server.py To test the server, we use httpie and httpx by default. You can have other choices but if you want to install them: pip install httpie httpx","title":"Get started"},{"location":"example/echo/","text":"An echo server is usually the very first server you wanna implement to get familiar with the framework. This server sleeps for a given period and return. It is a simple illustration of how multi-stage workload is implemented. It also shows how to write a simple validation for input data. The default JSON protocol will be used since the (de)serialization methods are not overridden in this demo. In particular, the input data of Preprocess 's forward is a dictionary decoded by JSON from the request body's bytes; and the output dictionary of Postprocess 's forward will be JSON-encoded as a mirrored process. echo.py \u00a4 import logging import time from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) class Preprocess ( Worker ): def forward ( self , data : dict ) -> float : logger . debug ( f \"pre received { data } \" ) # Customized, simple input validation try : time = float ( data [ \"time\" ]) except KeyError as err : raise ValidationError ( f \"cannot find key { err } \" ) return time class Inference ( Worker ): def forward ( self , data : float ) -> float : logger . info ( f \"sleeping for { data } seconds\" ) time . sleep ( data ) return data class Postprocess ( Worker ): def forward ( self , data : float ) -> dict : logger . debug ( f \"post received { data } \" ) return { \"msg\" : f \"sleep { data } seconds\" } if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess ) server . append_worker ( Inference ) server . append_worker ( Postprocess ) server . run () Start \u00a4 python echo.py Test \u00a4 http :8000/inference time=1.5","title":"Echo"},{"location":"example/echo/#echopy","text":"import logging import time from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) class Preprocess ( Worker ): def forward ( self , data : dict ) -> float : logger . debug ( f \"pre received { data } \" ) # Customized, simple input validation try : time = float ( data [ \"time\" ]) except KeyError as err : raise ValidationError ( f \"cannot find key { err } \" ) return time class Inference ( Worker ): def forward ( self , data : float ) -> float : logger . info ( f \"sleeping for { data } seconds\" ) time . sleep ( data ) return data class Postprocess ( Worker ): def forward ( self , data : float ) -> dict : logger . debug ( f \"post received { data } \" ) return { \"msg\" : f \"sleep { data } seconds\" } if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess ) server . append_worker ( Inference ) server . append_worker ( Postprocess ) server . run ()","title":"echo.py"},{"location":"example/echo/#start","text":"python echo.py","title":"Start"},{"location":"example/echo/#test","text":"http :8000/inference time=1.5","title":"Test"},{"location":"example/metric/","text":"This is an example demonstrating how to add your customized Python side Prometheus metrics. Mosec already has the Rust side metrics, including: throughput for the inference endpoint duration for each stage (including the IPC time) batch size (only for the max_batch_size > 1 workers) number of remaining tasks to be processed If you need to monitor more details about the inference process, you can add some Python side metrics. E.g., the inference result distribution, the duration of some CPU-bound or GPU-bound processing, the IPC time (get from rust_step_duration - python_step_duration ). This example has a simple WSGI app as the monitoring metrics service. In each worker process, the Counter will collect the inference results and export them to the metrics service. For the inference part, it parses the batch data and compares them with the average value. For more information about the multiprocess mode for the metrics, check the Prometheus doc . python_side_metrics.py \u00a4 import logging import os import pathlib import tempfile import threading from typing import List from wsgiref.simple_server import make_server from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) # check the PROMETHEUS_MULTIPROC_DIR environment variable before import Prometheus if not os . environ . get ( \"PROMETHEUS_MULTIPROC_DIR\" ): metric_dir_path = os . path . join ( tempfile . gettempdir (), \"prometheus_multiproc_dir\" ) pathlib . Path ( metric_dir_path ) . mkdir ( parents = True , exist_ok = True ) os . environ [ \"PROMETHEUS_MULTIPROC_DIR\" ] = metric_dir_path from prometheus_client import ( # type: ignore # noqa: E402 CONTENT_TYPE_LATEST , CollectorRegistry , Counter , generate_latest , multiprocess , ) metric_registry = CollectorRegistry () multiprocess . MultiProcessCollector ( metric_registry ) counter = Counter ( \"inference_result\" , \"statistic of result\" , ( \"status\" , \"worker_id\" )) def metric_app ( environ , start_response ): data = generate_latest ( metric_registry ) start_response ( \"200 OK\" , [( \"Content-Type\" , CONTENT_TYPE_LATEST ), ( \"Content-Length\" , str ( len ( data )))], ) return iter ([ data ]) def metric_service ( host = \"\" , port = 8080 ): with make_server ( host , port , metric_app ) as httpd : httpd . serve_forever () class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . worker_id = str ( self . id ) def deserialize ( self , data : bytes ) -> int : json_data = super () . deserialize ( data ) try : res = int ( json_data . get ( \"num\" )) except Exception as err : raise ValidationError ( err ) return res def forward ( self , data : List [ int ]) -> List [ bool ]: avg = sum ( data ) / len ( data ) ans = [ x >= avg for x in data ] counter . labels ( status = \"true\" , worker_id = self . worker_id ) . inc ( sum ( ans )) counter . labels ( status = \"false\" , worker_id = self . worker_id ) . inc ( len ( ans ) - sum ( ans ) ) return ans if __name__ == \"__main__\" : # Run the metrics server in another thread. metric_thread = threading . Thread ( target = metric_service , daemon = True ) metric_thread . start () # Run the inference server server = Server () server . append_worker ( Inference , num = 2 , max_batch_size = 8 ) server . run () Start \u00a4 python python_side_metrics.py Test \u00a4 http POST :8000/inference num=1 Check the Python side metrics \u00a4 http :8080 Check the Rust side metrics \u00a4 http :8000/metrics","title":"Metrics"},{"location":"example/metric/#python_side_metricspy","text":"import logging import os import pathlib import tempfile import threading from typing import List from wsgiref.simple_server import make_server from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) # check the PROMETHEUS_MULTIPROC_DIR environment variable before import Prometheus if not os . environ . get ( \"PROMETHEUS_MULTIPROC_DIR\" ): metric_dir_path = os . path . join ( tempfile . gettempdir (), \"prometheus_multiproc_dir\" ) pathlib . Path ( metric_dir_path ) . mkdir ( parents = True , exist_ok = True ) os . environ [ \"PROMETHEUS_MULTIPROC_DIR\" ] = metric_dir_path from prometheus_client import ( # type: ignore # noqa: E402 CONTENT_TYPE_LATEST , CollectorRegistry , Counter , generate_latest , multiprocess , ) metric_registry = CollectorRegistry () multiprocess . MultiProcessCollector ( metric_registry ) counter = Counter ( \"inference_result\" , \"statistic of result\" , ( \"status\" , \"worker_id\" )) def metric_app ( environ , start_response ): data = generate_latest ( metric_registry ) start_response ( \"200 OK\" , [( \"Content-Type\" , CONTENT_TYPE_LATEST ), ( \"Content-Length\" , str ( len ( data )))], ) return iter ([ data ]) def metric_service ( host = \"\" , port = 8080 ): with make_server ( host , port , metric_app ) as httpd : httpd . serve_forever () class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . worker_id = str ( self . id ) def deserialize ( self , data : bytes ) -> int : json_data = super () . deserialize ( data ) try : res = int ( json_data . get ( \"num\" )) except Exception as err : raise ValidationError ( err ) return res def forward ( self , data : List [ int ]) -> List [ bool ]: avg = sum ( data ) / len ( data ) ans = [ x >= avg for x in data ] counter . labels ( status = \"true\" , worker_id = self . worker_id ) . inc ( sum ( ans )) counter . labels ( status = \"false\" , worker_id = self . worker_id ) . inc ( len ( ans ) - sum ( ans ) ) return ans if __name__ == \"__main__\" : # Run the metrics server in another thread. metric_thread = threading . Thread ( target = metric_service , daemon = True ) metric_thread . start () # Run the inference server server = Server () server . append_worker ( Inference , num = 2 , max_batch_size = 8 ) server . run ()","title":"python_side_metrics.py"},{"location":"example/metric/#start","text":"python python_side_metrics.py","title":"Start"},{"location":"example/metric/#test","text":"http POST :8000/inference num=1","title":"Test"},{"location":"example/metric/#check-the-python-side-metrics","text":"http :8080","title":"Check the Python side metrics"},{"location":"example/metric/#check-the-rust-side-metrics","text":"http :8000/metrics","title":"Check the Rust side metrics"},{"location":"example/pytorch/","text":"Here are some out-of-the-box model servers powered by mosec for PyTorch users. We use the version 1.9.0 in the following examples. Natural Language Processing \u00a4 Natural language processing model servers usually receive text data and make predictions ranging from text classification, question answering to translation and text generation. Sentiment Analysis \u00a4 This server receives a string and predicts how positive its content is. We build the model server based on Transformers of version 4.11.0. We show how to customize the deserialize method of the ingress stage ( Preprocess ) and the serialize method of the egress stage ( Inference ). In this way, we can enjoy the high flexibility, directly reading data bytes from request body and writing the results into response body. Note that in a stage that enables batching (e.g. Inference in this example), its worker's forward method deals with a list of data, while its serialize and deserialize methods only need to manipulate individual datum. Server \u00a4 python distil_bert_sentiment.py distil_bert_sentiment.py import logging from typing import List , TypeVar import torch # type: ignore from transformers import ( # type: ignore AutoModelForSequenceClassification , AutoTokenizer , ) from mosec import Server , Worker T = TypeVar ( \"T\" ) logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) INFERENCE_BATCH_SIZE = 32 class Preprocess ( Worker ): def __init__ ( self ): super () . __init__ () self . tokenizer = AutoTokenizer . from_pretrained ( \"distilbert-base-uncased-finetuned-sst-2-english\" ) def deserialize ( self , data : bytes ) -> str : # Override `deserialize` for the *first* stage; # `data` is the raw bytes from the request body return data . decode () def forward ( self , data : str ) -> T : tokens = self . tokenizer . encode ( data , add_special_tokens = True ) return tokens class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) logger . info ( f \"using computing device: { self . device } \" ) self . model = AutoModelForSequenceClassification . from_pretrained ( \"distilbert-base-uncased-finetuned-sst-2-english\" ) self . model . eval () self . model . to ( self . device ) # Overwrite self.example for warmup self . example = [ [ 101 , 2023 , 2003 , 1037 , 8403 , 4937 , 999 , 102 ] * 5 # make sentence longer ] * INFERENCE_BATCH_SIZE def forward ( self , data : List [ T ]) -> List [ str ]: tensors = [ torch . tensor ( token ) for token in data ] with torch . no_grad (): result = self . model ( torch . nn . utils . rnn . pad_sequence ( tensors , batch_first = True ) )[ 0 ] scores = result . softmax ( dim = 1 ) . cpu () . tolist () return [ f \"positive= { p } \" for ( _ , p ) in scores ] def serialize ( self , data : str ) -> bytes : # Override `serialize` for the *last* stage; # `data` is the string from the `forward` output return data . encode () if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess ) server . append_worker ( Inference , max_batch_size = INFERENCE_BATCH_SIZE ) server . run () Client \u00a4 curl -X POST http://127.0.0.1:8000/inference -d 'i bought this product for many times, highly recommend' Computer Vision \u00a4 Computer vision model servers usually receive images or links to the images (downloading from the link becomes an I/O workload then), feed the preprocessed image data into the model and extract information like categories, bounding boxes and pixel labels as results. Image Recognition \u00a4 This server receives an image and classify it according to the ImageNet categorization. We specifically use ResNet as an image classifier and build a model service based on it. Nevertheless, this file serves as the starter code for any kind of image recognition model server. We enable multiprocessing for Preprocess stage, so that it can produce enough tasks for Inference stage to do batch inference , which better exploits the GPU computing power. More interestingly, we also started multiple model by setting the number of worker for Inference stage to 2. This is because a single model is hard to fully occupy the GPU memory or utilization. Multiple models running on the same device in parallel can further increase our service throughput. We also demonstrate how to customized validation on the data content through this example. In the forward method of the Preprocess worker, we firstly check the key of the input, then try to decode the str and load it into array. If any of these steps fails, we raise the ValidationError . The status will be finally returned to our clients as HTTP 422 . Server \u00a4 python resnet50_server.py resnet50_server.py import base64 import logging from typing import List from urllib.request import urlretrieve import cv2 # type: ignore import numpy as np # type: ignore import torch # type: ignore import torchvision # type: ignore from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) INFERENCE_BATCH_SIZE = 16 class Preprocess ( Worker ): def forward ( self , req : dict ) -> np . ndarray : # Customized validation for input key and field content; raise # ValidationError so that the client can get 422 as http status try : image = req [ \"image\" ] im = np . frombuffer ( base64 . b64decode ( image ), np . uint8 ) im = cv2 . imdecode ( im , cv2 . IMREAD_COLOR )[:, :, :: - 1 ] # bgr -> rgb except KeyError as err : raise ValidationError ( f \"cannot find key { err } \" ) except Exception as err : raise ValidationError ( f \"cannot decode as image data: { err } \" ) im = cv2 . resize ( im , ( 256 , 256 )) crop_im = ( im [ 16 : 16 + 224 , 16 : 16 + 224 ] . astype ( np . float32 ) / 255 ) # center crop crop_im -= [ 0.485 , 0.456 , 0.406 ] crop_im /= [ 0.229 , 0.224 , 0.225 ] crop_im = np . transpose ( crop_im , ( 2 , 0 , 1 )) return crop_im class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) logger . info ( f \"using computing device: { self . device } \" ) self . model = torchvision . models . resnet50 ( pretrained = True ) self . model . eval () self . model . to ( self . device ) # Overwrite self.example for warmup self . example = [ np . zeros (( 3 , 244 , 244 ), dtype = np . float32 ) ] * INFERENCE_BATCH_SIZE def forward ( self , data : List [ np . ndarray ]) -> List [ int ]: logger . info ( f \"processing batch with size: { len ( data ) } \" ) with torch . no_grad (): batch = torch . stack ([ torch . tensor ( arr , device = self . device ) for arr in data ]) output = self . model ( batch ) top1 = torch . argmax ( output , dim = 1 ) return top1 . cpu () . tolist () class Postprocess ( Worker ): def __init__ ( self ): super () . __init__ () logger . info ( \"loading categories file...\" ) local_filename , _ = urlretrieve ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" ) with open ( local_filename ) as f : self . categories = list ( map ( lambda x : x . strip (), f . readlines ())) def forward ( self , data : int ) -> dict : return { \"category\" : self . categories [ data ]} if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess , num = 4 ) server . append_worker ( Inference , num = 2 , max_batch_size = INFERENCE_BATCH_SIZE ) server . append_worker ( Postprocess , num = 1 ) server . run () Client \u00a4 python resnet50_client.py resnet50_client.py import base64 import httpx # type: ignore dog_bytes = httpx . get ( \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\" ) . content prediction = httpx . post ( \"http://localhost:8000/inference\" , json = { \"image\" : base64 . b64encode ( dog_bytes ) . decode ()}, ) if prediction . status_code == 200 : print ( prediction . json ()) else : print ( prediction . status_code , prediction . content )","title":"PyTorch"},{"location":"example/pytorch/#natural-language-processing","text":"Natural language processing model servers usually receive text data and make predictions ranging from text classification, question answering to translation and text generation.","title":"Natural Language Processing"},{"location":"example/pytorch/#sentiment-analysis","text":"This server receives a string and predicts how positive its content is. We build the model server based on Transformers of version 4.11.0. We show how to customize the deserialize method of the ingress stage ( Preprocess ) and the serialize method of the egress stage ( Inference ). In this way, we can enjoy the high flexibility, directly reading data bytes from request body and writing the results into response body. Note that in a stage that enables batching (e.g. Inference in this example), its worker's forward method deals with a list of data, while its serialize and deserialize methods only need to manipulate individual datum.","title":"Sentiment Analysis"},{"location":"example/pytorch/#server","text":"python distil_bert_sentiment.py distil_bert_sentiment.py import logging from typing import List , TypeVar import torch # type: ignore from transformers import ( # type: ignore AutoModelForSequenceClassification , AutoTokenizer , ) from mosec import Server , Worker T = TypeVar ( \"T\" ) logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) INFERENCE_BATCH_SIZE = 32 class Preprocess ( Worker ): def __init__ ( self ): super () . __init__ () self . tokenizer = AutoTokenizer . from_pretrained ( \"distilbert-base-uncased-finetuned-sst-2-english\" ) def deserialize ( self , data : bytes ) -> str : # Override `deserialize` for the *first* stage; # `data` is the raw bytes from the request body return data . decode () def forward ( self , data : str ) -> T : tokens = self . tokenizer . encode ( data , add_special_tokens = True ) return tokens class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) logger . info ( f \"using computing device: { self . device } \" ) self . model = AutoModelForSequenceClassification . from_pretrained ( \"distilbert-base-uncased-finetuned-sst-2-english\" ) self . model . eval () self . model . to ( self . device ) # Overwrite self.example for warmup self . example = [ [ 101 , 2023 , 2003 , 1037 , 8403 , 4937 , 999 , 102 ] * 5 # make sentence longer ] * INFERENCE_BATCH_SIZE def forward ( self , data : List [ T ]) -> List [ str ]: tensors = [ torch . tensor ( token ) for token in data ] with torch . no_grad (): result = self . model ( torch . nn . utils . rnn . pad_sequence ( tensors , batch_first = True ) )[ 0 ] scores = result . softmax ( dim = 1 ) . cpu () . tolist () return [ f \"positive= { p } \" for ( _ , p ) in scores ] def serialize ( self , data : str ) -> bytes : # Override `serialize` for the *last* stage; # `data` is the string from the `forward` output return data . encode () if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess ) server . append_worker ( Inference , max_batch_size = INFERENCE_BATCH_SIZE ) server . run ()","title":"Server"},{"location":"example/pytorch/#client","text":"curl -X POST http://127.0.0.1:8000/inference -d 'i bought this product for many times, highly recommend'","title":"Client"},{"location":"example/pytorch/#computer-vision","text":"Computer vision model servers usually receive images or links to the images (downloading from the link becomes an I/O workload then), feed the preprocessed image data into the model and extract information like categories, bounding boxes and pixel labels as results.","title":"Computer Vision"},{"location":"example/pytorch/#image-recognition","text":"This server receives an image and classify it according to the ImageNet categorization. We specifically use ResNet as an image classifier and build a model service based on it. Nevertheless, this file serves as the starter code for any kind of image recognition model server. We enable multiprocessing for Preprocess stage, so that it can produce enough tasks for Inference stage to do batch inference , which better exploits the GPU computing power. More interestingly, we also started multiple model by setting the number of worker for Inference stage to 2. This is because a single model is hard to fully occupy the GPU memory or utilization. Multiple models running on the same device in parallel can further increase our service throughput. We also demonstrate how to customized validation on the data content through this example. In the forward method of the Preprocess worker, we firstly check the key of the input, then try to decode the str and load it into array. If any of these steps fails, we raise the ValidationError . The status will be finally returned to our clients as HTTP 422 .","title":"Image Recognition"},{"location":"example/pytorch/#server_1","text":"python resnet50_server.py resnet50_server.py import base64 import logging from typing import List from urllib.request import urlretrieve import cv2 # type: ignore import numpy as np # type: ignore import torch # type: ignore import torchvision # type: ignore from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) INFERENCE_BATCH_SIZE = 16 class Preprocess ( Worker ): def forward ( self , req : dict ) -> np . ndarray : # Customized validation for input key and field content; raise # ValidationError so that the client can get 422 as http status try : image = req [ \"image\" ] im = np . frombuffer ( base64 . b64decode ( image ), np . uint8 ) im = cv2 . imdecode ( im , cv2 . IMREAD_COLOR )[:, :, :: - 1 ] # bgr -> rgb except KeyError as err : raise ValidationError ( f \"cannot find key { err } \" ) except Exception as err : raise ValidationError ( f \"cannot decode as image data: { err } \" ) im = cv2 . resize ( im , ( 256 , 256 )) crop_im = ( im [ 16 : 16 + 224 , 16 : 16 + 224 ] . astype ( np . float32 ) / 255 ) # center crop crop_im -= [ 0.485 , 0.456 , 0.406 ] crop_im /= [ 0.229 , 0.224 , 0.225 ] crop_im = np . transpose ( crop_im , ( 2 , 0 , 1 )) return crop_im class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) logger . info ( f \"using computing device: { self . device } \" ) self . model = torchvision . models . resnet50 ( pretrained = True ) self . model . eval () self . model . to ( self . device ) # Overwrite self.example for warmup self . example = [ np . zeros (( 3 , 244 , 244 ), dtype = np . float32 ) ] * INFERENCE_BATCH_SIZE def forward ( self , data : List [ np . ndarray ]) -> List [ int ]: logger . info ( f \"processing batch with size: { len ( data ) } \" ) with torch . no_grad (): batch = torch . stack ([ torch . tensor ( arr , device = self . device ) for arr in data ]) output = self . model ( batch ) top1 = torch . argmax ( output , dim = 1 ) return top1 . cpu () . tolist () class Postprocess ( Worker ): def __init__ ( self ): super () . __init__ () logger . info ( \"loading categories file...\" ) local_filename , _ = urlretrieve ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" ) with open ( local_filename ) as f : self . categories = list ( map ( lambda x : x . strip (), f . readlines ())) def forward ( self , data : int ) -> dict : return { \"category\" : self . categories [ data ]} if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess , num = 4 ) server . append_worker ( Inference , num = 2 , max_batch_size = INFERENCE_BATCH_SIZE ) server . append_worker ( Postprocess , num = 1 ) server . run ()","title":"Server"},{"location":"example/pytorch/#client_1","text":"python resnet50_client.py resnet50_client.py import base64 import httpx # type: ignore dog_bytes = httpx . get ( \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\" ) . content prediction = httpx . post ( \"http://localhost:8000/inference\" , json = { \"image\" : base64 . b64encode ( dog_bytes ) . decode ()}, ) if prediction . status_code == 200 : print ( prediction . json ()) else : print ( prediction . status_code , prediction . content )","title":"Client"}]}